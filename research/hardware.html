<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-06-01 Sat 12:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Henry Robinson" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0b64f18">1. <span class="todo TODO">TODO</span> In-depth OOO execution details</a>
<ul>
<li><a href="#org91702b5">1.1. <span class="todo TODO">TODO</span> Tomosuolo algorithm</a></li>
<li><a href="#org7566b8c">1.2. <span class="todo TODO">TODO</span> What's a reservation station?</a></li>
</ul>
</li>
<li><a href="#org74abe1f">2. <b>Modern Microprocessors - A 90-minute guide</b></a>
<ul>
<li><a href="#org781c3dc">2.1. Pipelining and instruction-level parallelism</a></li>
<li><a href="#org92982be">2.2. Deeper pipelines - <b>superpipelining</b></a></li>
<li><a href="#orgb699faf">2.3. Multiple issue - <b>superscalar</b></a></li>
<li><a href="#orga2ac713">2.4. Explicit parallelism - VLIW</a></li>
<li><a href="#org0abcfcb">2.5. Instruction dependencies and latencies</a></li>
<li><a href="#org386ade3">2.6. Branches and branch prediction</a></li>
<li><a href="#orge1307d2">2.7. Eliminating branches with predication</a></li>
<li><a href="#orga06637d">2.8. Instruction scheduling, register renaming &amp; OOO</a></li>
<li><a href="#org178bbc2">2.9. The <span class="underline">Brainiac Debate</span></a></li>
<li><a href="#orgb7665fd">2.10. The Power Wall &amp; The ILP Wall</a></li>
<li><a href="#org5a9ac61">2.11. What about x86?</a></li>
<li><a href="#org36eb410">2.12. Threads - SMT, Hyper-Threading and Multi-Core</a></li>
<li><a href="#org699c0da">2.13. More cores or wider cores?</a></li>
<li><a href="#org8232309">2.14. Data Parallelism - SIMD Vector Instructions</a></li>
<li><a href="#org96b4945">2.15. Memory and the memory wall</a></li>
<li><a href="#org8fecc3c">2.16. Caches and the memory hierarchy</a></li>
</ul>
</li>
<li><a href="#org008c4c3">3. <b>FPGA architecture</b></a>
<ul>
<li><a href="#orgbaf6ede">3.1. Reprogrammable logic gates</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0b64f18" class="outline-2">
<h2 id="org0b64f18"><span class="section-number-2">1</span> <span class="todo TODO">TODO</span> In-depth OOO execution details</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org91702b5" class="outline-3">
<h3 id="org91702b5"><span class="section-number-3">1.1</span> <span class="todo TODO">TODO</span> Tomosuolo algorithm</h3>
</div>
<div id="outline-container-org7566b8c" class="outline-3">
<h3 id="org7566b8c"><span class="section-number-3">1.2</span> <span class="todo TODO">TODO</span> What's a reservation station?</h3>
</div>
</div>
<div id="outline-container-org74abe1f" class="outline-2">
<h2 id="org74abe1f"><span class="section-number-2">2</span> <b>Modern Microprocessors - A 90-minute guide</b></h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li><a href="http://www.lighterra.com/papers/modernmicroprocessors/">http://www.lighterra.com/papers/modernmicroprocessors/</a></li>
</ul>
</div>
<div id="outline-container-org781c3dc" class="outline-3">
<h3 id="org781c3dc"><span class="section-number-3">2.1</span> Pipelining and instruction-level parallelism</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Instructions executed in pipeline.</li>
<li>Typical four stage pipeline:
<ul class="org-ul">
<li>Fetch instruction from memory</li>
<li>Decode instruction
<ul class="org-ul">
<li>this means decompose instruction into microcode, compute jump target, read
register values into latches, etc.</li>
</ul></li>
<li>Execute instruction</li>
<li>Writeback results to registers, memory etc.</li>
</ul></li>
<li>With a full pipeline, processor will retire one instruction every cycle
<ul class="org-ul">
<li>Clocks-per-instruction (CPI) = 1</li>
</ul></li>
<li>Pipeline stages are linked by latches which are synchronized by clock signal.</li>
<li>Two consecutive instructions A and B, where B depends on output of A:
<ul class="org-ul">
<li>B would be at EXECUTE stage when A is performing WRITEBACK</li>
<li>So if B needs output of A, pipeline has to stall for one clock</li>
<li>Instead, pipeline can write results of execute to latch inputs of B's execute phase
<ul class="org-ul">
<li>(and also from writeback, to avoid having to read registers)</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org92982be" class="outline-3">
<h3 id="org92982be"><span class="section-number-3">2.2</span> Deeper pipelines - <b>superpipelining</b></h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Clock speed limited by length of slowest stage in pipeline (so makes sense to make
pipeline stages short)</li>
<li>This means more cycles per instruction, but more cycles per second so higher
throughput</li>
<li>Modern pipelines are about 15-20 stages deep
<ul class="org-ul">
<li>Pentium 4 went nuts with 30+ stages</li>
<li>x86 processors have to do more work to support CISC instruction set than RISC CPUs.</li>
</ul></li>
<li>Remember - deep pipelines -&gt; higher clock speed</li>
</ul>
</div>
</div>
<div id="outline-container-orgb699faf" class="outline-3">
<h3 id="orgb699faf"><span class="section-number-3">2.3</span> Multiple issue - <b>superscalar</b></h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Lots of different functional units, not all in use for a given instruction (e.g. ALU,
FPU). Why not issue multiple instructions to take advantage of idle hardware?</li>
<li>Now fetch / decode stages need to be able to do multiple instructions in parallel.</li>
<li>Might be able to retire more than one instruction per clock (i.e. CPI &lt; 1)</li>
<li>"issue width" -&gt; Number of instructions in flight at one time.</li>
<li>Virtually every processor is superpipelined and superscalar, so just called superscalar
for short.</li>
</ul>
</div>
</div>
<div id="outline-container-orga2ac713" class="outline-3">
<h3 id="orga2ac713"><span class="section-number-3">2.4</span> Explicit parallelism - VLIW</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>Idea is to design instruction set to explicitly group instructions to be executed in
parallel.</li>
<li>i.e. instructions themselves indicate to CPU that there are few or no dependencies.</li>
<li>Instructions are 128 bits or more, hence "Very Long Instruction Word"</li>
<li>Fetch and decode happens once every word, but execute / writeback happens multiple times
per instruction.</li>
<li>No need for parallel fetch or decode, so simpler.</li>
<li>Most designs not "interlocked", i.e. no dependency checking is done between instructions.
<ul class="org-ul">
<li>As a result, compiler has to insert manual NOPs to stall pipeline between dependent
instructions.</li>
<li>Not clear why - maybe complexity of dependency checking for VLIWs is higher?</li>
<li>Or maybe dependency checking happens in frontend and complexity savings of VLIW would
be moot with it?</li>
</ul></li>
<li>Itanium was VLIW. GPU shader chips are sometimes VLIW.</li>
</ul>
</div>
</div>

<div id="outline-container-org0abcfcb" class="outline-3">
<h3 id="org0abcfcb"><span class="section-number-3">2.5</span> Instruction dependencies and latencies</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Why not build 50-stage pipeline which issues 20 instructions per clock?</li>
<li>Answer: dependencies.</li>
<li>In a long pipeline, the number of stages between starting EXECUTE and results being
available can be very high. That is the number of clock cycles that dependent
instructions would need to be stalled for.</li>
<li>So length of pipeline eventually diminishes its returns.</li>
</ul>
</div>
</div>
<div id="outline-container-org386ade3" class="outline-3">
<h3 id="org386ade3"><span class="section-number-3">2.6</span> Branches and branch prediction</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>Another problem for pipelining. CPU doesn't know which instruction is next, so has to
stall FETCH until previous instruction writes IP.</li>
<li>So processor guesses based on branch predictor. Output of speculated instructions not
committed until previous instruction is retired (I think this doesn't require stalling
though - previous instruction will be at WRITEBACK when current instruction is in
EXECUTE, so WRITEBACK of current instruction will know whether or not to discard
results)</li>
<li>Misprediction penalty can be very high, as it means an entire pipeline stall.</li>
</ul>
</div>
</div>
<div id="outline-container-orge1307d2" class="outline-3">
<h3 id="orge1307d2"><span class="section-number-3">2.7</span> Eliminating branches with predication</h3>
<div class="outline-text-3" id="text-2-7">
<ul class="org-ul">
<li>If target of branch is basically one instruction, possible to have that instruction
combined with branch logic, so no speculation required.</li>
<li>e.g. cmovle d, b ; if result of compare is &lt;=, b = d
<ul class="org-ul">
<li>executes in one instruction, no ambiguity about fetching next instruction</li>
<li>I suppose there's still a one-cycle penalty if branch not taken because nothing is
retired usefully from cmovle.</li>
</ul></li>
<li><span class="underline">Larger example</span>:
cmp a, 7
mov c, b
cmovle d, b
<ul class="org-ul">
<li>Means if (a &lt;= 7) b =d else b = c.</li>
<li>Execute the 'else' branch <b>whatever</b> (mov c, b), and then overwrite it using
conditional move. This increases parallelism - cmp and mov can be executed in parallel
as only the cmovle has a dependency on either.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga06637d" class="outline-3">
<h3 id="orga06637d"><span class="section-number-3">2.8</span> Instruction scheduling, register renaming &amp; OOO</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li>If there are bubbles in the pipeline, what can be done to put those idle stages to good
work? One possibility - issue instructions <b>Out Of Order</b> so that while one instruction
is stalled, others can execute.</li>
<li>One way to do this is to reorder in the hardware at runtime. Decode / dispatch logic
must be extended to look at groups of instructions and dispatch them as best it can
given possible dependencies. This is <b>Out Of Order Execution</b> or <b>OOO</b>.
<ul class="org-ul">
<li>To keep track of dependencies between instructions, <b>rename registers</b> so that hazards
are explicit (and reduce false-positives where there's benign dependencies on same
register).</li>
<li>OOO, dependency analysis, register renaming add a lot of complex logic - making CPU
larger, hotter, harder to design and more power-hungry.</li>
<li>But OOO means that software need not be recompiled to get more fine-grained
parallelism.</li>
</ul></li>
<li>Alternatively: have compiler rearrange instructions. Processor can be <b>in-order</b> on the
assumption that the compiler gives the best instruction stream.
<ul class="org-ul">
<li>Other advantages - compiler can see further down program than hardware, and can
speculate down multiple branches (CPUs can typically do just one).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org178bbc2" class="outline-3">
<h3 id="org178bbc2"><span class="section-number-3">2.9</span> The <span class="underline">Brainiac Debate</span></h3>
<div class="outline-text-3" id="text-2-9">
<ul class="org-ul">
<li>Is OOO really worth it?
<ul class="org-ul">
<li><b>Brainiacs</b> vs <b>speed-demons</b>
<ul class="org-ul">
<li>Braniacs complex, clever</li>
<li>Speed-demons simple, smaller
<ul class="org-ul">
<li>(Speed-demons could run at higher clock speeds, but clock now limited by power and
thermal issues)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Speed-demons can usually fit more cores on a chip. So are 4 brainiacs faster than 8
speed-demons?</li>
<li>Debate is ongoing - benefits and costs of OOO have been overstated in past.
<ul class="org-ul">
<li>Powerl, clock overheads of OOO have been reduced by engineering</li>
<li>Chip area is still an issue.</li>
<li>Effectiveness of OOO surprisingly disappointing - only maybe 20-40% improvement over
in-order design.
<ul class="org-ul">
<li><b>"The dirty little secret of OOO is that we are often not very much OOO at all"</b></li>
</ul></li>
</ul></li>
<li>Vendors have often gone down one path, then switched.
<ul class="org-ul">
<li>For example, DEC Alpha and MIPS: speed-demon -&gt; brainiac</li>
<li>Sparc moved from brainiac -&gt; speed-demon</li>
<li>Intel most interesting:
<ul class="org-ul">
<li>x86 has to be a little bit brainiac due to complexities of architecture</li>
<li>Pentium Pro was full brainiac.</li>
<li>Then AMD wars turned focus to speed at all cost, and Pentium 4 was highly
speed-demon
<ul class="org-ul">
<li>Pentium 4 had 20-30 stage pipeline to get up to 3.8GHz (!!)</li>
</ul></li>
<li>Itanium was speed-demon, hoping for smart-compilers</li>
<li>However, Itanium failed, Pentium 4 had heat and power issues and was beaten by
Athlons at roughly half the clock speed</li>
<li>So Intel went brainiac on Core processors.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb7665fd" class="outline-3">
<h3 id="orgb7665fd"><span class="section-number-3">2.10</span> The Power Wall &amp; The ILP Wall</h3>
<div class="outline-text-3" id="text-2-10">
<ul class="org-ul">
<li>Power usage goes up faster than clock speed</li>
<li>W goes up proportional to V*V, and V goes up with clock speed because transistors need
more power to meet higher switching requirements</li>
<li>Result is that 30%+ clock speed can double power requirements (and double heat produced)</li>
<li>So just pumping up clock speed can't work forever</li>
<li>But neither can going full brainiac - there isn't that much ILP in a single program..
<ul class="org-ul">
<li>Average ILP of modern CPU running SPECint is &lt; 2 instructions per cycle (&gt; 0.5 CPI)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5a9ac61" class="outline-3">
<h3 id="org5a9ac61"><span class="section-number-3">2.11</span> What about x86?</h3>
<div class="outline-text-3" id="text-2-11">
<ul class="org-ul">
<li>x86 instruction set is CISC and messy - complex addressing modes and not many registers
makes it hard to parallelize instructions because false-positives on dependencies</li>
<li>So <b>dynamically decode</b> x86 instructions into simple, RISC-like micro-instructions and
execute using simple RISC-style core
<ul class="org-ul">
<li>x86 instructions typically decode into 1, 2 or 3 micro-ops.</li>
<li>Static instruction scheduling might be less effective because compiler can't see into RISC core.</li>
<li>Can't fix small register set this way.</li>
<li>Pentium Pro was first Intel chip to do this; today all x86 processors do.</li>
</ul></li>
<li>Some recent chips store uops in a small cache to avoid retranslating during loops.
<ul class="org-ul">
<li>So pipeline stages can be skipped; e.g. Haswell is a 14/19 stage processor. 14 when
running from uop cache, 19 when not.</li>
</ul></li>
<li>This makes talking about the issue width of a CPU a bit tricky (because width should be
measured in uops not instructions)</li>
<li>Even more tricky - some uops get 'fused' into a single uop
<ul class="org-ul">
<li>Presumably this reduces register pressure, and increases uop throughput</li>
<li>Per Agner Fog (<a href="http://www.agner.org/optimize/microarchitecture.pdf">http://www.agner.org/optimize/microarchitecture.pdf</a>, pg 107) - fused
uops act as one instruction everywehere but during execution (where they are sent to
different execution units independently)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org36eb410" class="outline-3">
<h3 id="org36eb410"><span class="section-number-3">2.12</span> Threads - SMT, Hyper-Threading and Multi-Core</h3>
<div class="outline-text-3" id="text-2-12">
<ul class="org-ul">
<li>Since ILP is actually rather limited, maybe we can take instructions from some other
thread?</li>
<li>Multiplex more than one thread onto a single core.</li>
<li>Requires duplicating state tracking part of core (e.g. instruction pointer, registers,
TLB etc), but not the large parts like decoders, dispatch logic etc.
<ul class="org-ul">
<li>Overhead is ~10%</li>
</ul></li>
<li>Since no dependency between threads, there's lots of parallelism available.</li>
<li>Except&#x2026;
<ul class="org-ul">
<li>Not always lots of CPU-bound threads running at same time</li>
<li>Threads share the same cache</li>
<li>Other resources also shared (e.g. functional units) - if one thread saturates all of a
functional unit, no parallelism available for other thread (e.g. heavy FPU workloads)</li>
</ul></li>
<li>So SMT could actually be worse than single-thread perf. But usually a big win in context
of memory latencies etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org699c0da" class="outline-3">
<h3 id="org699c0da"><span class="section-number-3">2.13</span> More cores or wider cores?</h3>
<div class="outline-text-3" id="text-2-13">
<ul class="org-ul">
<li>Why build multiple cores if an SMT design would be better?
<ul class="org-ul">
<li>Very wide superscalar designs scale very badly</li>
<li>For example, dispatch logic scales with square of issue width, because you have to
compare n*n instruction pairs to be sure about dependencies.</li>
<li>Very wide superscalar also requires multiple ports on register files and caches, to
allow for parallel access.</li>
<li>All this implies more wiring, larger chip size, etc.</li>
<li>So a 10-issue core would possibly be larger and slower than 2*5-issue cores.</li>
</ul></li>
<li>So there's a sweet spot between SMT width and number of cores.
<ul class="org-ul">
<li>e.g. core2 -&gt; 4 cores, 6-issue OOO brainiac cores</li>
<li>or Niagara 3 -&gt; 16 simple 2-issue in-order cores</li>
<li>or Bulldozer - shared, SMT-style front-end for a pair of cores, but back-end is
unshared for integers, and shared for floating-point.</li>
</ul></li>
<li>There are lots of transistors (~6 billion) available. So lots of room to fit logic on a
chip.
<ul class="org-ul">
<li>Although now pressure to integrate more into chip, like SoC.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org8232309" class="outline-3">
<h3 id="org8232309"><span class="section-number-3">2.14</span> Data Parallelism - SIMD Vector Instructions</h3>
<div class="outline-text-3" id="text-2-14">
<ul class="org-ul">
<li>Along with ILP and SMT, a further way to get parallelism is <b>data parallelism</b>.</li>
<li>i.e. run one instruction on a group of data values in parallel.</li>
<li>SIMD instructions can pack multiple values into single registers and operate on them
using a single register operation.</li>
</ul>
</div>
</div>
<div id="outline-container-org96b4945" class="outline-3">
<h3 id="org96b4945"><span class="section-number-3">2.15</span> Memory and the memory wall</h3>
<div class="outline-text-3" id="text-2-15">
<ul class="org-ul">
<li>Access to main memory is very slow.</li>
<li>Reading a byte can cost ~20 cycles of main memory bus, which can lag CPU clock by a
factor of three (e.g. 800MHz to 2400MHz)</li>
<li>So effective cost can be 60 cycles (plus cache checking delay) or more of CPU clock to
access main memory.</li>
<li>As ratio of clock speed to bus speed gets larger, delay gets worse.</li>
</ul>
</div>
</div>
<div id="outline-container-org8fecc3c" class="outline-3">
<h3 id="org8fecc3c"><span class="section-number-3">2.16</span> Caches and the memory hierarchy</h3>
<div class="outline-text-3" id="text-2-16">
<ul class="org-ul">
<li>Caches hide latency. For example, on a core i4:
<ul class="org-ul">
<li>L1 cache -&gt; 4 cycles</li>
<li>L2 cache -&gt; 12 cycles</li>
<li>L3 cache -&gt; 21 cycles</li>
<li>RAM -&gt; 120 cycles</li>
</ul></li>
<li>Temporal and spatial locality make caches highly effective.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org008c4c3" class="outline-2">
<h2 id="org008c4c3"><span class="section-number-2">3</span> <b>FPGA architecture</b></h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgbaf6ede" class="outline-3">
<h3 id="orgbaf6ede"><span class="section-number-3">3.1</span> Reprogrammable logic gates</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Logic gates are look-up tables (truth tables) which can be reprogrammed
Connected by fixed-width bus
Very high parallelism
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Henry Robinson</p>
<p class="date">Created: 2019-06-01 Sat 12:35</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
